{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why do we need dimensionality reduction???\n",
    "\n",
    "Having multiple features increases the \"exploration\" space, if we have $N_{1}$ possible values for feature 1 and $N_{2}$ possible values for feature 2 then we can have a total of $N_{1} \\times N_{2}$ combinations. If we continue until getting $n$ features we would have a total of $N_{1}\\times N_{2} \\times ... \\times N_{n}$ possible values, so the space increases exponentially with the number of features we have. \n",
    "\n",
    "Dimensionality reduction, as it names says, reduces the dimension of the feature space. In this specific notebook we will see the Principle Component Analysis (PCA) which uses linear algebra to find multiple linear combinations of features that have greatest impact in the feature-space. This is done as follows:\n",
    "\n",
    "1. Transform the dataset $X$ such that the values of each feature is centered at 0, this can be easily done by substracting the mean of each feature to their corresponding column or by using a standardized transformation that gets the data to have mean 0 and standard deviation of 1.\n",
    "\n",
    "2. Get the covariance matrix:\n",
    "$$\\text{cov}(X) = \\frac{X^{T}X}{n-1}$$\n",
    "\n",
    "where $n$ are the number of rows and $X$ is the dataset with shape $nxm$ where $m$ are the number of features.\n",
    "\n",
    "3. Calculate the eigenvectors and eigenvalues of the system:\n",
    "$$\\text{cov}(X)\\vec{p_{a}} = \\lambda_{a}\\vec{p_{a}}$$\n",
    "\n",
    "i.e., the eigenvectors $\\vec{p_{a}}$ which remain invariant after the operation of the covariance matrix and their corresponding eigenvalues $\\lambda_{a}$\n",
    "\n",
    "4. Transform the data by multiplying it by the matrix with the $N$ eigenvectors that got the greatest value.\n",
    "$$X_{transformed} = X\\cdot V_{N}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a dataset from sklearn.datasets.make_blob\n",
    "## Create sample dataset with sklearn \n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_samples = 1000\n",
    "n_features = 5\n",
    "\n",
    "data =  make_blobs(n_samples = n_samples, \n",
    "                   n_features = n_features, \n",
    "                   centers = 1,\n",
    "                   random_state = 12)\n",
    "feature_data = data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Center data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_data = feature_data - feature_data.mean(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get covariance matrix (normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.99607584, -0.02450582,  0.02846781,  0.01337188, -0.03843541],\n",
       "       [-0.02450582,  1.01683351,  0.00938314,  0.01686747, -0.02826945],\n",
       "       [ 0.02846781,  0.00938314,  0.99091033, -0.02925299,  0.00343573],\n",
       "       [ 0.01337188,  0.01686747, -0.02925299,  1.04513819,  0.02365677],\n",
       "       [-0.03843541, -0.02826945,  0.00343573,  0.02365677,  1.0432768 ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "cov = np.transpose(center_data)@center_data/(len(center_data)-1) # This division is what normalize the covariance\n",
    "cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get eigenvectors and eigenvalues of covariance matrix (normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EigResult(eigenvalues=array([0.92650081, 1.07989176, 1.00115088, 1.03113944, 1.05355178]), eigenvectors=array([[ 0.60542414,  0.30527195, -0.2389985 , -0.68016109, -0.1432873 ],\n",
       "       [ 0.39135557,  0.09934794, -0.39782425,  0.64159948, -0.51677537],\n",
       "       [-0.48934024,  0.26033264, -0.80391314, -0.07854956,  0.20081504],\n",
       "       [-0.31891962, -0.54831639, -0.12010783, -0.3441484 , -0.68174438],\n",
       "       [ 0.37301317, -0.72699042, -0.35201329,  0.03350894,  0.45531296]]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eig_sys = np.linalg.eig(cov)\n",
    "eig_sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the data\n",
    "\n",
    "We will pick the top 3 eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.34601898,  3.9805265 ,  7.99370598],\n",
       "       [-2.38634649,  3.71377939,  7.60816424],\n",
       "       [-4.45121296,  5.09147019,  7.6649479 ],\n",
       "       ...,\n",
       "       [-3.81499632,  5.64604836,  8.22068892],\n",
       "       [-3.50139373,  1.15440944,  5.81720655],\n",
       "       [-4.76827168,  4.12086166,  6.3339521 ]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_transf = feature_data@eig_sys[1][:, :3]\n",
    "X_transf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataScience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
